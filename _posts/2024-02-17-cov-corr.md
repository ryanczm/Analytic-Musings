---
layout: post
title: "Intuiting Correlation, Covariance, Portfolio Risk/Return and Simple Regression"
category: quant
excerpt: "I decided to grok correlation, covariance, beta, portfolio variance (risk & return, diversification) and single-variable regression by focusing on simulation to gain intuition for how a thing works by plugging in numbers. To quote: thereâ€™s no shame in the rest of us figuring it out by poking at a thing from different angles and seeing how it wiggles in response."

---

Taking a break from paper replications, I really wanted to grok or intuit some fundamental concepts, really built on expectation, variance and covariance. The two applied contexts are regression and portfolio risk/return. This is just baby steps, as in the future, I am going to do the same, but for proper multiple regression, regularized regression (ridge and lasso), and convex optimization.

So, I first grok variance, then covariance, then beta, then portfolio variance (diversification and correlation) and lastly look at how the simple regression form relates to the multiple regression, matrix form.  We do so by plugging in lots of different numbers and plotting things out. I should have done this long ago, but better late than never. 

# Variance
---
The key point is that the formula for variance can be expressed in two ways: directly, as expectation of squared distance from expectation, and by expanding the formula out and manipulating the expectation operator:

$$Var(x)=\mathbb{E}[(X-\mathbb{E}(x)(X-\mathbb{E}(x)]=\mathbb{E}[X^2]-\mathbb{E}[X]\mathbb{E}[X]= A-B$$

The key intuition is that once we expand out the first formula to get the second, the second formula is much more informative. Variance is simply the gap between $A$ and $B$, or how much more $A$ is over $B$.

When $E[X]=0$, then variance is just the mean of the squared array $E[X^2]$. We square the array, element-wise, to ensure negative deviations don't cancel out positive ones. We can visualise the gap like this:

<center>
<img src="{{ site.imageurl }}/CovCorrReg/1.png" style="width:65%;"/>
</center>


So, variance is simply the gap between the 'variance' term and the demeaning offset term to account for means. Thus, shifting variance by changing mean intuitively doesn't change it - the 'variance' term and offset term increase/decrease by equal amounts:

```python
s = generate_normal_df(0, 2, 1000)
print(f"e(x^2): {s.xx.mean().round(3)}")
print(f"e(x)^2: {(s.x.mean() * s.x.mean()).round(3)}")
---
> e(x^2): 3.743
> e(x)^2: 0.001

s = generate_normal_df(100, 2, 1000)
print(f"e(x^2): {s.xx.mean().round(3)}")
print(f"e(x)^2: {(s.x.mean() * s.x.mean()).round(3)}")
---
> e(x^2): 10019.331
> e(x)^2: 10015.507
```

Now for scaling variance. We know from expanding the formula that variance scales as a square: $Var(2x)=4 \cdot Var(x)$ or $Var(\frac{1}{2}x)=\frac{1}{4} \cdot Var(x)$. To be honest, the best intuition for this is expanding the formula itself, so not much to do there.  

<center>
<img src="{{ site.imageurl }}/CovCorrReg/2.png" style="width:65%;"/>
</center>


---
# Covariance

The expanded formula really shines when it comes to covariance: 

$$Cov(x,y)=\mathbb{E}[(X-\mathbb{E}(x)(Y-\mathbb{E}(y)]=\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]= A-B$$

Using the previous intuition, the $E[X]E[Y]$ term is the offset term again, so the main information is contained in $E[XY]$ and the gap of $A$ over $B$. We can gain intuition by plotting out two joint normal random variables with a given correlation on a scatter, and looking at the values of the product array $XY$. Since we've demonstrated the shift intuition, we set $E[X]=E[Y]=0$.  

### Uncorrelated

<center>
<img src="{{ site.imageurl }}/CovCorrReg/3.png" style="width:55%;"/>
</center>


Now looking at $XY$, we see it's a good mix of positive and negatives. This makes sense, since conditioning the second realization on the first $Y\|X$ does nothing to change it, when say, $X$ is very negative, $Y$ could be anything with 'equal' chance, positive, negative or 0.

<center>
<img src="{{ site.imageurl }}/CovCorrReg/4.png" style="width:90%;"/>
</center>


### Positive 


<center>
<img src="{{ site.imageurl }}/CovCorrReg/5.png" style="width:55%;"/>
</center>


Looking at $XY$, if $X$ is negative, then $Y$ is likely to be negative too, which cancels out. Thus $XY$ should be very positive. Since $B=0$, $E[XY]>0$ and the  $A$ term is positive.

<center>
<img src="{{ site.imageurl }}/CovCorrReg/6.png" style="width:90%;"/>
</center>


### Negative

<center>
<img src="{{ site.imageurl }}/CovCorrReg/7.png" style="width:55%;"/>
</center>


Looking at $XY$, if $X$ is negative, then $Y$ is likely to be positive. Thus $XY$ should be very negative. Since $B=0$, $E[XY]<0$ and the $B$ term is negative.

<center>
<img src="{{ site.imageurl }}/CovCorrReg/8.png" style="width:90%;"/>
</center>

So, the $E[XY]$ term controls the covariance while $E[X]E[Y]$ accounts for shifts (mean).

---
# Intuiting Beta and Correlation

Correlation is the covariance, accounting for scale:

$$corr(x,y)=\frac{E[XY]-E[X]E[Y]}{\sigma_x \sigma_y}$$

From this formula, correlation accounts for shift and scale in a symmetric fashion: if $\sigma_x$ > $\sigma_y$, the denominator takes care of it. However, $\beta$ is asymmetric:

$$\beta_{Y|X}=\frac{Cov(X,Y)}{Var(X)}=\frac{Cov(X,Y)}{\sigma_x\sigma_x}=\frac{Cov(X,Y)}{\sigma_x \sigma_y} \frac{\sigma_y}{\sigma_x}=corr(X,Y) \frac{\sigma_y}{\sigma_x}$$

Thus, $\beta$ is the correlation (symmetric), scaled by the ratio of volatilities to become asymmetric. If $\sigma_x >> \sigma_y$, meaning the $x$ is much more spread out than $y$, then even with a high correlation, the slope will be flat. This is easy to see with a scatterplot. 


<center>
<img src="{{ site.imageurl }}/CovCorrReg/9.png" style="width:70%;"/>
</center>

Thus, a unit increase in $x$ is less in terms of it's spread than $y$, and beta reflects that accordingly. So, beta is just correlation scaled by the scale ratio.

---
# Pearson's Correlation vs $R^2$ in Simple Regression

In simple regression, the $R^2$, or coefficient of determination, is $1-\frac{SSR}{SST}$. Intuitively, to get to $1$ from below, then the 'penalty' term should be as small as possible. The error is scaled by the total variance in $y$ to account for scale.

Another interpretation is that it is the squared correlation coefficient of $y$ against $\hat{y}$. A proof is shown [here](https://economictheoryblog.com/2014/11/05/proof/). However, the key intuition is that correlation is a function of the target and features, while $R^2$ is a function of the target and model.

 To see this, imagine two sets of data, with one predictor and target each: one where there is high correlation and $R^2$ and another where there is no correlation but high $R^2$. In the first scenario: $x$ is correlated to $y$, so there is high correlation and $R^2$.  In the latter scenario, $x$ is uniform but $y$ is roughly constant, so the predictor is uncorrelated with the target, but a simple naive fit captures majority of variance.

$$x$$


---
# Sums of Correlated Random Variables

To do this, first we need to intuit why the sum of two correlated random variables have higher variance. To me, expanding the formula $Var(X+Y)$ and deriving the covariance terms doesn't provide much intuition. So we plot it out: first we generate some correlated random variables, then plot the histogram of $x+y$ vs $2x$. From the scatterplots, we saw that while the joints were different each time under different correlation, the marginals were the same.

In the negative case, the sums $x_i+y_i$ are more likely to cancel each other out, so that's fine. In the positive case, the sums $x_i+y_i$ are less likely to cancel each other out compared to the uncorrelated case. You simply get less cancellations. 

```python
s = generate_correlated_normals(-0.8,0,1,0,1,1000)
s['x_'] = np.random.normal(0,1,1000)
s['2x'] = s.x + s.x_
s['x+y'] = s.x + s.y

s.var()
---
> x      0.980820
> y      0.982434
> xy     1.802900
> x_     0.996730
> 2x     1.914183
> x+y    3.543132
> dtype: float64
```

We see the variance of $2x$ is just the sum, but in the $x+y$ case, there is an extra 'boost' from less cancellations.


<center>
<img src="{{ site.imageurl }}/CovCorrReg/10.png" style="width:65%;"/>
</center>

And the opposite effect happens for negatively correlated random variables:

<center>
<img src="{{ site.imageurl }}/CovCorrReg/11.png" style="width:65%;"/>
</center>

---
# Covariance Matrix as Linear Transformation on White Data (Eigenvector Stuff)  

* Generate white noise (2 by n)
* Generate covariance matrix (2 by 2)
* Calculate applied covariance matrix by hand on white noise (2 by 2) for intuition.
* Calculate applied covariance matrix in code on white noise (2 by n)
* Plot scatter
* Do eigendecomposition
* Relate how eigenvector/values transform white noise on scatter.


---
# Intuiting Portfolio Variance/Diversification

With that, we can intuit why negatively correlated assets can improve the Sharpe ratio by reducing risk much more than return. First, we generate 3 scenarios of 3 asset returns: uncorrelated, positive and negative.

```python
mu = np.array([0.05, 0.05, 0.05])
sigma = np.array([0.25, 0.25, 0.25])

uncorr = np.eye(3)

poscorr = np.array([[1.0, 0.8, 0.6], 
                    [0.8, 1.0, 0.6], 
                    [0.6, 0.6, 1.0]])  

negcorr = np.array([[1.0, -0.2, -0.3], 
                    [-0.2, 1.0, 0.1], 
                    [-0.3, 0.1, 1.0]])  

# Number of observations
n = 1000

# Generate asset returns for three scenarios
ur = generate_asset_returns(mu, sigma, uncorr, n)
pr = generate_asset_returns(mu, sigma, poscorr, n)
nr = generate_asset_returns(mu, sigma, negcorr, n)
```

We can then find the tangency portfolio weights for each one:

```
Optimal Weights (Positively correlated assets): [0.30195395 0.29755717 0.40048888]
Sharpe: 0.7374
Return 0.0539
Vol 0.046
---
Optimal Weights (Uncorrelated Assets): [0.28991556 0.3454199  0.36466454]
Sharpe: 1.6746
Return 0.0554
Vol 0.0211
---
Optimal Weights (Negatively correlated Assets): [0.36839998 0.33702856 0.29457145]
Sharpe: 1.625
Return 0.0427
Vol 0.014
```

The returns are very similar but the volatilities are very different, leading to different Sharpe ratios.

<center>
<img src="{{ site.imageurl }}/CovCorrReg/12.png" style="width:60%;"/>
</center>

Regenerating and tweaking the values slightly, we can then sample over many runs to plot the efficient frontier and risk-return profiles for bootstrapped weights.

```python
mu = np.array([0.02, 0.01, 0.03])
sigma = np.array([0.25, 0.43, 0.6])

uncorr = np.eye(3)
poscorr = np.array([[1.0, 0.4, 0.3], 
                    [0.4, 1.0, 0.6], 
                    [0.3, 0.6, 1.0]])  
negcorr = np.array([[1.0, -0.2, -0.3], 
                    [-0.2, 1.0, 0.1], 
                    [-0.3, 0.1, 1.0]])  

uncov = np.diag(sigma).T @ uncorr @ np.diag(sigma)
poscov = np.diag(sigma).T @ poscorr @ np.diag(sigma)
negcov = np.diag(sigma).T @ negcorr @ np.diag(sigma)
plot_efficient_frontier(mu, poscov, uncov, negcov)
```
The efficient frontier is shifted left for different return profile, for each risk level, negative > uncorrelated > positive produces the highest returns in that order.

<center>
<img src="{{ site.imageurl }}/CovCorrReg/13.png" style="width:80%;"/>
</center>

---
# Simple Linear Regression to Matrix Form

Lastly, I would like to intuit the link between simple linear regression and the matrix form. 

$$\beta_0=\bar{y} - \beta_1 \bar{x} \quad \quad  \beta_1 = \frac{\bar{xy}- \bar{x}\bar{y}}{\bar{x^2}-\bar{x}^2} \quad \Longleftrightarrow \quad \textbf{B} = (\textbf{X}^T\textbf{X})^{-1} \textbf{X}^T \textbf{Y} $$

The inverse corresponds to division by $Var$, fair enough. But both $Cov$ and $Var$ are demeaned and then divided by $n$, but the matrices are not! What gives? Well dividing by $n$ cancels on both sides, so it's just the demeaning part. Apparently, adding the conforming vector of ones $\textbf{1}$ to $\textbf{X}$ is the key. This "orthogonalizes the vector with respect to a constant term":
<br>
<center>
<img src="{{ site.imageurl }}/CovCorrReg/spe.png" style="width:80%;"/>
</center>
<br>
To do this, we can use a simple toy example with both numbers and symbols. For $y=0.5x+2$, we have $y=[3,4]$ and $x=[2,4]$.

$E[X]=3$, $E[X^2]=10$, $Var(X)=1$, $E[Y]=3.5$, $E[XY]=11$ and $Cov(X,Y)=0.5$ and so $\beta_1=0.5$. Now in matrix form:

$$(X^TX)^{-1}X^TY = (\begin{bmatrix} 1 & 1 \\ 2 & 4 \end{bmatrix}\begin{bmatrix} 1 & 2 \\ 1 & 4 \end{bmatrix})^{-1} \begin{bmatrix} 1 & 1 \\ 2 & 4 \end{bmatrix} \begin{bmatrix} 3 \\ 4 \end{bmatrix} $$

$$=\frac{1}{4}\begin{bmatrix} 20 & -6 \\ -6 & 2 \end{bmatrix}\begin{bmatrix} 7 \\ 22 \end{bmatrix}$$

$$=\frac{1}{4}\begin{bmatrix} 8 \\ 2 \end{bmatrix}=\begin{bmatrix} 2 \\ 0.5 \end{bmatrix}$$

Okay, but that doesn't tell us the intuition, only that it works. In symbols, both $(\textbf{X}^T\textbf{X})^{-1}$ and $\textbf{X}^T \textbf{Y}$ are divided by $\frac{1}{n}$. This is taken from [CMU 36-401 Lecture 13](https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/13/lecture-13.pdf):

$$
\begin{align}

(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} &= \frac{1}{s^2_{X}}
\begin{bmatrix}
    \bar{x^2} & -\bar{x} \\
    -\bar{x} & 1
\end{bmatrix}
\begin{bmatrix}
    \bar{y} \\
    \bar{xy}
\end{bmatrix} \tag{30} \\

&= \frac{1}{s^2_{X}}
\begin{bmatrix}
    \bar{x^2}\bar{y} - \bar{x}\bar{xy} \\
    -\bar{x}\bar{y} + \bar{xy}
\end{bmatrix} \tag{31} \\

&= \frac{1}{s^2_{X}}
\begin{bmatrix}
    (s^2_{X} + \bar{x}^2)\bar{y} - \bar{x}(C_{xy} + \bar{x}\bar{y}) \\
    C_{xy}
\end{bmatrix} \tag{32} \\

&= \frac{1}{s^2_{X}}
\begin{bmatrix}
    s^2_{X}\bar{y} + \bar{x}^2\bar{y} - C_{xy}\bar{x} - \bar{x}^2\bar{y} \\
    C_{xy}
\end{bmatrix} \tag{33}\\


&= \begin{bmatrix}
    \bar{y} - \frac{C_{xy}}{s^2_{X}} \bar{x}\\
    \frac{C_{xy}}{s^2_{X}}
\end{bmatrix} \tag{34} \\

\end{align}


$$

Note from $(31)$ to $(32)$, the formulas for covariance and variance in terms of $E[XY]$ and $E[X^2]$ are substituted in respectively. And there it is! The $\textbf{1}$'s vector is key.