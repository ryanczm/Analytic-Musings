---
image: "/./assets/images/FWL/cover/cover.png"
layout: post
title: "Intuiting the Frisch-Waugh-Lovell (FWL) Theorem"
category: regression
excerpt: "In this article, I take a look at the Frisch-Waugh-Lovell (FWL) theorem, which lets us understand the interpretation of individual betas in a multiple regression after partialling out correlation effects, and lets us view the intercept as orthogonalizing the features with regard to a constant vector. Featuring stuff from Gregory Gundersen, RyxCommar."
---

<style>
  .twitter-tweet {
    width: 30%;  /* Adjust the width as needed */
    margin: 0 auto;  /* Center the blockquote horizontally */
  
  }

    .twitter-tweet p {
    font-size: 14px !important; /* Change the font size */
  }

</style>

So after doing some regression self-study, inspired by [_Chief Ridge Officer Quantymacro_](https://x.com/quantymacro), I decided to write a post to consolidate some of my learnings. To give context, these guys are part of _QuantTwit_, a collection of quants on $\mathbb{X}$. Now if there's anything I learned from reading $\mathbb{X}$, it's that quants have _tremendous respect_ for regression.

The nature of the work involves quantifying relationships between noisy, high-dim, collinear, non-stationary data, and regression fits the bill. The FWL theorem lets us spruce up our interpretation of regression in a few ways that are useful.

<center>
<img src="{{ site.imageurl }}/FWL/1a.png" style="width:70%;"/>
</center>

<!-- <blockquote class="twitter-tweet" data-conversation="none" data-theme="light"><p lang="en" dir="ltr">To understand FWL is to understand a key intuition of what multiple regression is actually doing.</p>&mdash; Senior PowerPoint Engineer (@ryxcommar) <a href="https://twitter.com/ryxcommar/status/1673459506058084352?ref_src=twsrc%5Etfw">June 26, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> -->

Disclaimer: I'm just a scrub. Please feel free to correct mistakes. 

## Motivating the Theorem


It's worth noting the theorem is less related to the computational aspects of regression (no matrix decomps like SVD involved in the derivation). I'll have a look at ridge regression and the SVD as per it's implementation in `statsmodels` in a future post.

The theorem can be motivated by this question:

<!-- <center>
<img src="{{ site.imageurl }}/FWL/2a.png" style="width:65%;"/>
</center> -->

<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">More generally, *all* the effects estimated by OLS are orthogonalized.<br><br>Here&#39;s are 3 fun data science interview questions relating to this:<br><br>(1.) You add a new X variable to your regression, and you see that the other coefficients change. Why is that?</p>&mdash; Senior PowerPoint Engineer (@ryxcommar) <a href="https://twitter.com/ryxcommar/status/1718436394333327553?ref_src=twsrc%5Etfw">October 29, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Intuitively, you would say _"if the new predictor is correlated to other predictors, the coefficients might change"_. Or conversely, _"if the new predictor is uncorrelated/orthogonal to the current ones, the coefficients remain the same"_.

The FWL theorem formalizes this notion. It states the individual beta of a feature in a multiple regression is a partial effect: the effect of that variable after residualising/orthogonalising out the influence of the other predictors. If we have:

$$Y=\beta_1X_1+ \beta_2X_2+\epsilon$$ 

and want to isolate $\beta_1$, we can first regress $y \sim X_2$ and $X_1 \sim X_2$ and take their residuals, then regress first set of residuals on the second. FWL tells us the slope of this regression is our isolated beta $\beta_1$: 

$$r(y \sim X_2)=\beta_1 r(X_1 \sim X_2)$$

This 'leave-one-out', 'residual-on-residual' style set of regressions can be done for each beta. But why residual?

The residual of $\hat{y}{\| X_2}$ is the remaining value of our $y$ that cannot be 'explained' by $X_2$. The residual $\hat{X_1}{\| X_2}$ is the remaining part of the $X_1$ feature that is orthogonal to $X_2$. Regress the two sets of residuals, and you have found the 'pure' relationship between $X_1$ and $Y$ after partialling out correlations - _orthogonalizing_ $y$ and $X_1$ w.r.t $X_2$. 

So _in theory_, assessing a single beta in a multiple regression, one need not worry about it's correlation to other features because it's correlation has been partialled out! _In practice_, this is not the case, but that's why ridge and SVD are used.

## The Proof

I'm going to cite the proof from *Gregory Gundersen's blog* in his awesome [_OLS article_](https://gregorygundersen.com/blog/2020/01/04/ols/). 

<center>
<img src="{{ site.imageurl }}/FWL/gg.png" style="width:70%;"/>
</center>

First, rewrite the OLS equation into partitioned form:

<center>
<img src="{{ site.imageurl }}/FWL/3a.png" style="width:90%;"/>
</center>

Relate it back to the normal equation for betas $(\textbf{X}^T\textbf{X})\hat{\beta}=\textbf{X}^T\textbf{y}$:

<center>
<img src="{{ site.imageurl }}/FWL/3b.png" style="width:90%;"/>
</center>

Expand it out. This uses the outer product to multiply the first two matrices together, then transform the beta vector to give two equations.

<center>
<img src="{{ site.imageurl }}/FWL/3c.png" style="width:90%;"/>
</center>

Solve for $\hat{\beta_1}$:

<center>
<img src="{{ site.imageurl }}/FWL/3d.png" style="width:90%;"/>
</center>

Substitute this expression for $\hat{\beta_1}$ into the second equation:

<center>
<img src="{{ site.imageurl }}/FWL/3e.png" style="width:90%;"/>
</center>


Define the hat and residual maker matrices. The hat matrix *orthogonally projects* the endogenous vector onto the column space of the design matrix. Hastie & Tibshirani illustrate this in §3.2.3 of ESL which is the first image in this post. The hat and residual maker matrices are orthogonal.

<center>
<img src="{{ site.imageurl }}/FWL/3f.png" style="width:90%;"/>
</center>


Put them into the equation and solve for $\hat{\beta_2}$ in terms of residual-on-residual regression.

<center>
<img src="{{ site.imageurl }}/FWL/3g.png" style="width:90%;"/>
</center>

Rearrange the terms:

<center>
<img src="{{ site.imageurl }}/FWL/3h.png" style="width:90%;"/>
</center>

This expression is the normal equation for regressing $\textbf{y}$ on $\textbf{X}_2$ but with $\textbf{M}_1$ in front of $\textbf{y}$ and certain $\textbf{X}_2$ terms. Why not in front of the first $\textbf{X}_2$? Because $\textbf{M}_1$ is an orthogonal projection aka $\textbf{M}=\textbf{M}^2$ and also orthogonal so $\textbf{M}=\textbf{M}^T$. 

So the residual-on-residual idea of FWL can be expressed in terms of the residual maker matrix $\textbf{M}_1\textbf{y} = \textbf{M}_1\textbf{X}_2\hat{\beta_2} + \textbf{e}$.



## Why Is This Useful

But who cares? Why is this useful? Here are some reasons:

* The interpretation of individual $\hat{\beta}_i$ after partialling out correlation effects. 
* Relates geometry (orthogonalisation/residualisation) to removing correlation (assuming linear relationship). 
* Viewing `sm.add_constant(X)` to the design matrix for an intercept as partitioned regression where $\textbf{X}_1=\textbf{1}$, equivalent to demeaning.

Tree-based ML models (rf, xgb, etc) don't have this neat interpretation of coefficients! It would be interesting to compare feature importances of a tree-based model like xgboost to the betas from linear regression both fit on the same data.

<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">Essentially, subtracting the mean, i.e. centering the vector, is the same thing as orthogonalizing with respect to a constant term.<br><br>This is how we get to Var(x), and not just simply x².<br><br>Sweet! We now see how (X&#39;X)⁻¹ and 1/Var(x) are related.</p>&mdash; Senior PowerPoint Engineer (@ryxcommar) <a href="https://twitter.com/ryxcommar/status/1718434683032150444?ref_src=twsrc%5Etfw">October 29, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


Again, this derivation is from _Gundersen_. In simple regression, the intercept is the difference of two means, effectively de-meaning the target and predictor:

$$\hat{\alpha}=\bar{y}-\hat{\beta}\bar{x}$$

Why would you want an intercept? This image from Gundersen's blog shows why:

<center>
<img src="{{ site.imageurl }}/FWL/4a.png" style="width:90%;"/>
</center>


So you do a parallel shift of your regression slope upwards by a constant coefficient to account for differences in means. Anyway, in the multiple regression case, we set $\textbf{M}_1$ where $X_1=\textbf{1}$ and compute $\beta_1$

$$\textbf{y}= \textbf{1}\beta_1 + \textbf{X}_2\beta_2 +\textbf{e}$$

The hat matrix $\textbf{H}_1$ expands out to become a $1/N$ matrix. Note an error in the picture, it should be $(\textbf{1}^T\textbf{1})^{-1}$.

<center>
<img src="{{ site.imageurl }}/FWL/5a.png" style="width:90%;"/>
</center>

This $\textbf{1}$ hat matrix when applied to the design matrix effectively produces column-wise means in every entry. To see this, use the inner product view of matrix multiplication. 

<center>
<img src="{{ site.imageurl }}/FWL/5b.png" style="width:90%;"/>
</center>

And it acts on the design matrix and the vector. So our $\hat{\beta}_2$ is effectively a demean and a regression:

<center>
<img src="{{ site.imageurl }}/FWL/5c.png" style="width:90%;"/>
</center>


Now use the equation for $\beta{1}$ that was derived earlier with $\textbf{X}_1=\textbf{1}$. And of course, the difference-in-means is captured in the $\hat{\beta{1}}$ as the hat matrix demeans the terms, and we see the intercept as a generalized case as the simple OLS intercept.

<center>
<img src="{{ site.imageurl }}/FWL/6a.png" style="width:90%;"/>
</center>


I think it's worth understanding what the intercept does, especially later in the case of multilevel regression/mixed effects models where you can have random intercepts, random slopes or both - see [_0xfdf's post on using multilevel regression to create a signal_](https://x.com/0xfdf/status/1801071181971235138).

And in commodities, there might be lots of chances to use mixed effects models (e.g predict some quantity across countries or different geographical areas).

## The Case Study

Let's look at a made-up example to see the FWL theorem in action. _This example is purely for illustration purposes_. Suppose we want to predict attractiveness from height and face, and these quantities are all z-scores, and the true relationship is linear: 

$$Attr \sim 1 Face + 0.4 Height + \epsilon$$

That is, face contributes to attractiveness ~ twice that of height. However, face is highly collinear to height (0.7). We can simulate this dataset.

```python
import numpy as np
import pandas as pd
import statsmodels.formula.api as smf
np.random.seed(69)

n = 50
rho = 0.7
beta1, beta2 = 1, 0.4

cov = np.array([1,rho,rho,1]).reshape(2,2)
mu = np.array([0,0])

exog = np.random.multivariate_normal(mu, cov, size=n)
endog = beta1 * exog.T[0] + beta2 * exog.T[1] + np.random.normal(0, 0.1, size=n)

df = pd.DataFrame(np.column_stack((exog, endog)), columns=['face', 'height', 'attr'])

hf = smf.ols('height ~ face', data=df).fit()
fh = smf.ols('face ~ height', data=df).fit()
ah = smf.ols('attr ~ height', data=df).fit()
af = smf.ols('attr ~ face', data=df).fit()
```


Plotting individual simple regressions of face and height, we see the individual betas are over-estimated because of the correlation between the two features.

<center>
<img src="{{ site.imageurl }}/FWL/7a.png" style="width:100%;"/>
</center>

Now let's isolate our height beta. We orthogonalize height on face and attractiveness on face. 

<center>
<img src="{{ site.imageurl }}/FWL/7b.png" style="width:100%;"/>
</center>

We are left with the residuals, the variation of height and attractiveness that are orthogonal to face.

<center>
<img src="{{ site.imageurl }}/FWL/7c.png" style="width:100%;"/>
</center>

Regressing the orthogonalized (w.r.t face) variables (residuals), demeaned, against each other, we recover the original height beta in the true relationship ($\pm$ the standard error):
<center>
<img src="{{ site.imageurl }}/FWL/7d.png" style="width:50%;"/>
</center>

## Further Stuff

That concludes this short article! For further investigation, I'd very much like to know how this works ...

<blockquote class="twitter-tweet" data-conversation="none"><p lang="en" dir="ltr">&quot;Question 8: Describe the difference between iterated regression and matrix decomp (rotation) to transform an N x M matrix so that each of the N columns is pairwise orthogonal.&quot;</p>&mdash; fdf (@0xfdf) <a href="https://twitter.com/0xfdf/status/1815162606069412124?ref_src=twsrc%5Etfw">July 21, 2024</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

But I'll work on that slowly.

For now, the FWL theorem provides us with essential intuition about key facets of multiple regression without going into any implementation details yet. 

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">To understand FWL is to understand a key intuition of what multiple regression is actually doing.</p>&mdash; Senior PowerPoint Engineer (@ryxcommar) <a href="https://twitter.com/ryxcommar/status/1673459506058084352?ref_src=twsrc%5Etfw">June 26, 2023</a>
</blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

I'll look at ridge regression (from [_QM articles_](https://www.quantymacro.com/advanced-ridge-regression/) of course) and how `statsmodels` implements it under the hood with SVD. In particular, to see how it deals with collinearity, and how to determine regularization strength. 